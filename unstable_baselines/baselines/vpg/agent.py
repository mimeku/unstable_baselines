from operator import itemgetter

import torch
import gym

from unstable_baselines.common import util
from unstable_baselines.common.agents import BaseAgent
from unstable_baselines.common.networks import MLPNetwork
from unstable_baselines.common.networks import PolicyNetworkFactory
from unstable_baselines.common.networks import get_optimizer


class VPGAgent(torch.nn.Module, BaseAgent):
    """ Vanilla Policy Gradient Agent
    https://proceedings.neurips.cc/paper/2001/file/4b86abe48d358ecf194c56c69108433e-Paper.pdf

    BaseAgent Args
    --------------
    observation_space: gym.Space

    action_sapce: gym.Space

    kwargs Args
    -----------
    gamma: float, default: 0.99
        Discount factor.

    train_v_iters: int, default: 80
        The number of times that the state-value network is updated in the agent.update 
        function, while the policy network is only updated once.

    action_bound_method: str, optional("clip", "tanh"), default: "clip"
        Method for mapping the raw action generated by policy network to the environment 
        action space.
    """

    def __init__(self,
                 observation_space: gym.Space,
                 action_space: gym.Space,
                 **kwargs):
        
        super(VPGAgent, self).__init__()
        # save parameters
        self.args = kwargs
        
        self.observation_space = observation_space
        self.action_space = action_space
        obs_dim = observation_space.shape[0]
        action_dim = action_space.shape[0]

        # initialize networks and optimizer
        self.v_network = MLPNetwork(obs_dim, 1, **kwargs['v_network']).to(util.device)
        self.v_optimizer = get_optimizer(kwargs['v_network']['optimizer_class'], self.v_network, kwargs['v_network']['learning_rate'])
        self.pi_network = PolicyNetworkFactory.get(obs_dim, action_space, **kwargs['pi_network']).to(util.device)
        self.pi_optimizer = get_optimizer(kwargs['pi_network']['optimizer_class'], self.pi_network, kwargs['pi_network']['learning_rate'])

        # register networks
        self.networks = {
            'v_network': self.v_network,
            'pi_network': self.pi_network
        }

        # hyper-parameters
        self.gamma = kwargs['gamma']
        self.train_v_iters = kwargs['train_v_iters']
        self.action_bound_method = kwargs['action_bound_method']

    def select_action(self, state):
        """ Select an action according to the passed state.

        Return
        ------
        raw_action: (action_dim, )
        log_prob: float
        """
        state = self.env_numpy_to_device_tensor(state)
        with torch.no_grad():
            # action: action_pre_tanh
            # log_prob: log_prob_pre_tanh
            action, log_prob = itemgetter('action_prev_tanh', 'log_prob_prev_tanh')(
                self.pi_network.sample(state)
            )
        return self.device_tensor_to_env_numpy(action, log_prob)

    def estimate_value(self, state):
        """ Estimate the state value.
        """
        state = self.env_numpy_to_device_tensor(state)
        with torch.no_grad():
            value = self.v_network(state)
        return value.detach().cpu().numpy()
    
    def update(self, data_batch: dict):
        """ Update the policy network and the state value network.

        Args
        ----
        data_batch: dict
            obs, act, ret, adv, logp
        """
        obs = data_batch['obs']
        act = data_batch['act']
        ret = data_batch['ret']
        adv = data_batch['adv']
        logp = data_batch['logp']
        
        # Train policy with a single step of gradient descent
        log_prob, entropy = self.pi_network.evaluate_actions(obs, act, action_type='raw')

        loss_pi = -(log_prob * adv).mean()
        self.pi_optimizer.zero_grad()
        loss_pi.backward()
        self.pi_optimizer.step()

        # Train value function
        for i in range(self.train_v_iters):
            estimation = self.v_network(obs)
            loss_v = ((estimation - ret)**2).mean()
            self.v_optimizer.zero_grad()
            loss_v.backward()
            self.v_optimizer.step()

        return {
            "loss/policy": loss_pi,
            "loss/v": loss_v
        }
